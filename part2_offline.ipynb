{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2_offline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYPRGHfPD7FTi8FBvMx5FN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdempsey/AI4Health-Online-Experimentation/blob/main/part2_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMH8jI_13OX"
      },
      "source": [
        "# Section 2: Synthetic HeartSteps and batch RL\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhbY9u0mwXb"
      },
      "source": [
        "## Import necessary \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qWBt8x2BEW"
      },
      "source": [
        "#Part 1: Overview on Contextual Bandits\n",
        "\n",
        "- For each person in a study, let $t=1,\\ldots, T$ denote a sequence of decision points.  \n",
        "- At each decision time $t$,  we observe a state variable $S_t \\in \\mathbb{R}^p$.  \n",
        "- After observing the state variable $S_t$, the _agent_ decides to take action $A_t \\in \\mathcal{A}$.  \n",
        "- After observing state $S_t$ and taking action $A_t$, the agent receive a reward $R_t$ given by\n",
        "$$\n",
        "R_t = r(S_t, A_t) + \\epsilon_t\n",
        "$$\n",
        "where $r(c,a)$ is a function that maps the state-action pair onto the real line and $\\epsilon_t$ is a random error term, e.g., $\\mathbb{E} [\\epsilon_t] = 0$. \n",
        "- The triple (context, action, reward) at a sequence of decision points defines a _contextual bandit_ setting.  \n",
        "- Here, the goal is to maximize the expected reward at every time point $\\mathbb{E}[R_t \\mid S_t, A_t=a] = r(S_t, a)$. \n",
        "- If we knew the reward function $r: \\mathcal{S} \\times \\mathcal{S} \\to \\mathbb{R}$, then the optimal action given state $s$ is\n",
        "$$\n",
        "a^\\star (s) = \\max_{a \\in \\mathcal{A}} r(s, a)\n",
        "$$\n",
        "\n",
        "### A simple approach:\n",
        "\n",
        "- Consider $\\mathcal{A} =\\{0,1\\}$\n",
        "- Randomize treatment $A_t \\sim \\text{Bern}(p)$ for $t=1,\\ldots,T$\n",
        "- Then for $t>T$, just choose\n",
        "$$\n",
        "\\hat A^\\star_t = \\max_{a \\in A} \\hat r(S_t, a)\n",
        "$$\n",
        "where $\\hat r(s,a)$ is the model fit using the batch data collected.\n",
        "- This is exactly equivalent to running an MRT and then using the data to construct an optimal decision rule based on the regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwolFIRmvIqC"
      },
      "source": [
        "## Question 1: What aspects of the reward impact decision making?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJQPaKw8Ash"
      },
      "source": [
        "- The only thing that impacts decision to choose $a = 1$ or $a=0$ is the _advantage function_:\n",
        "$$\n",
        "A(s) = r(s,1) - r(s,0)\n",
        "$$\n",
        "- In the synthetic example from previous section, we have\n",
        "$$\n",
        "A(s) = 0.5 s_{0} - 0.7 s_{1} > 0 \\Rightarrow \\frac{0.5}{0.7} s_0 > s_1\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnRinwV8DSb"
      },
      "source": [
        "## Question 2: What are the pros of this simple approach?  What are the cons?  \n",
        "\n",
        "- Why may we not want to use a randomized policy to collect data in mobile health?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrZnOLpMKQI"
      },
      "source": [
        "Pros (non-exhaustive)\n",
        "- Simple algorithm\n",
        "- With sufficient data will construct a 'good' policy\n",
        "- Easy to explain \n",
        "\n",
        "Cons (non-exhaustive)\n",
        "- Exploration is random so we learn slowly about the space\n",
        "- Exploit policy may not be optimal\n",
        "- How do we know that we collected enough data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejXfi7AvMYD"
      },
      "source": [
        "# Part 1b: Investigating mHealth randomized trial data\n",
        "\n",
        "- In HeartSteps V2, decision points are 6 times per day.  \n",
        "- An MRT simulator based on Heartsteps V2 has been built in R and is available [here](https://drive.google.com/drive/folders/1rhCWugawTjEnwmagrOPwxNssrgIsnypT?usp=sharing)\n",
        "\n",
        "\n",
        "The __State variable__ includes\n",
        "- __ID__: Numeric id taking values between 1-110\n",
        "- __Day__: Day-in-study (numeric)\n",
        "- __Decision time__: Numeric indicator of decision time per day (1-5)\n",
        "- __Dosage/burden__: Pre-defined function of past pushes (walking + anti-sedentary messages), prior to the current decision time. If there is any message delivered to user's phone (not just intent to treat) between time t and time t+1, e.g., active message at time t and anti-sedentary message between time t and t+1,  the dosage at time t+1 ($X_{t+1}$) is defined as $\\lambda \\cdot X_{t} + 1$. Otherwise, $X_{t+1} = \\lambda * X_{t}$, $\\lambda = 0.95$ set by the analysis of HS V1.\n",
        "- __Engagement Indicator__: Binary indicator of whether the number of screens encountered in app from prior day from 12am to 11:59pm is greater than the 40% quantile of the screens collected.\n",
        "- __Temperature__: Temperature (In Celsius degree) at the current location\n",
        "- __Location__: 1 if at a location other than home or work; 0 if at home or work (pre-specified)\n",
        "- __Variation Indicator__: For each time slot, first calculate the standard deviation of the (possibly imputed) 60-min steps  over the past 7 days.  Let the variation indicator on study day (d+1) to be 1 if the standard deviation calculated at day d is greater or equal to the median of the standard deviations up to day d in the study (excluding the warm up period), where d > 0.\n",
        "- __Pre-treatment Steps__: Log-transformed steps 30 mins prior to the current decision time from the tracker; $\\log(y+0.5)$.\n",
        "- __Square root of steps yesterday__: The square root of step counts from the tracker collected from 12am to 11:59 pm \n",
        "\n",
        "Below we show how to bring the MRT data back into python from the Google drive using [these instructions](https://colab.research.google.com/drive/1cMmtzM7rYc-cpW0fkRiTRb-ySr2UHf1h#scrollTo=XTFHRtl68d40).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oai1YoKBmaX9"
      },
      "source": [
        "import collections\r\n",
        "\r\n",
        "import glob\r\n",
        "\r\n",
        "# Importing drive method from colab for accessing google drive\r\n",
        "from google.colab import drive"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxK8XPHH6eNr"
      },
      "source": [
        " ## Importing Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYgqz-X_4zCX",
        "outputId": "773e5d74-7ad6-412e-c1aa-d35f28d15fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mounting drive\r\n",
        "# This will require authentication : Follow the steps as guided\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IPgr1qW40eR",
        "outputId": "39196d07-8c39-4eb2-920f-9c5fb2f86c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "HS_MRT_data = pd.read_csv(\"/content/drive/My Drive/ai4health/MRT_example.csv\")\r\n",
        "HS_MRT_data.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>day</th>\n",
              "      <th>decision.time</th>\n",
              "      <th>dosage</th>\n",
              "      <th>engagement</th>\n",
              "      <th>other.location</th>\n",
              "      <th>variation</th>\n",
              "      <th>temperature</th>\n",
              "      <th>logpresteps</th>\n",
              "      <th>sqrt.totalsteps</th>\n",
              "      <th>prior.anti</th>\n",
              "      <th>reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.644681</td>\n",
              "      <td>0.655219</td>\n",
              "      <td>0.492962</td>\n",
              "      <td>0</td>\n",
              "      <td>2.972736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.765377</td>\n",
              "      <td>0.625827</td>\n",
              "      <td>0.492962</td>\n",
              "      <td>0</td>\n",
              "      <td>5.150110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.808704</td>\n",
              "      <td>0.572993</td>\n",
              "      <td>0.492962</td>\n",
              "      <td>0</td>\n",
              "      <td>7.533758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1.902500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.827079</td>\n",
              "      <td>0.591051</td>\n",
              "      <td>0.492962</td>\n",
              "      <td>0</td>\n",
              "      <td>6.708086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2.807375</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.725725</td>\n",
              "      <td>0.538053</td>\n",
              "      <td>0.492962</td>\n",
              "      <td>0</td>\n",
              "      <td>6.937458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  day  decision.time  ...  sqrt.totalsteps  prior.anti    reward\n",
              "0   1    1              1  ...         0.492962           0  2.972736\n",
              "1   1    1              2  ...         0.492962           0  5.150110\n",
              "2   1    1              3  ...         0.492962           0  7.533758\n",
              "3   1    1              4  ...         0.492962           0  6.708086\n",
              "4   1    1              5  ...         0.492962           0  6.937458\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnmrB37Vv1t"
      },
      "source": [
        "## Part 1c: Going beyond Bandits (Batch V-Learning)\n",
        "\n",
        "- The above pre-supposed our goal is to construct a policy that maximizes the proximal outcome at each decision time\n",
        "- However, decisions at one time may impact the state at a future time\n",
        "- __Question__: What are some examples of delayed \n",
        "\n",
        "- In this case, we may want to maximize a different objective function.\n",
        "- Here, we consider the maximizing the state-value function,\n",
        "$$\n",
        "V(\\pi, s) = \\sum_{k \\geq 0} \\gamma^k \\mathbb{E}_\\pi \\left[ R_{t+k} \\mid S_t = s \\right]\n",
        "$$\n",
        "where $E_{\\pi}$ denotes the \n",
        "- However, we collected data under an MRT policy, $\\mu$.  So we need to re-express in terms of \n",
        "\\begin{align*}\n",
        "V(\\pi , s) &= \\sum_{k \\geq 0} \\mathbb{E}_\\mu \\left[ \\gamma^k R_{t+k} \\left\\{ \\prod_{v=0}^k \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\right\\} \\mid S_t = s \\right]  \\\\\n",
        "&= \\mathbb{E}_\\pi \\left[ \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\left( R_t + \\gamma \\sum_{k \\geq 0} \\mathbb{E}_\\pi \\left[ \\gamma^k R_{t+k+1} \\left\\{ \\prod_{v=0}^k \\frac{\\pi(A_{v+t+1}; S_{v+t+1})}{\\mu(A_{v+t+1}; S_{v+t+1})} \\right\\} \\mid S_{t+1} \\right] \\right) \\mid S_t \\right] \\\\\n",
        "&= \\mathbb{E}_\\pi \\left[ \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\left( R_t + \\gamma V(\\pi, S_{t+1}) \\right) \\mid S_t \\right] \\\\\n",
        "\\end{align*}\n",
        "This implies that\n",
        "$$\n",
        "0 = \\mathbb{E}_\\pi \\left[ \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\left( R_t + \\gamma V(\\pi, S_{t+1}) - V(\\pi, S_t) \\right) \\mid S_t \\right]\n",
        "$$\n",
        "in particular, for any $\\psi$ defined on the domain of $S$, the state-value function satisfies\n",
        "$$\n",
        "0 = \\mathbb{E}_\\pi \\left[ \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\left( R_t + \\gamma V(\\pi, S_{t+1}) - V(\\pi, S_t) \\right) \\psi (S_t) \\mid S_t \\right]\n",
        "$$\n",
        "This is an importance-weighted variant of the __Bellman optimality equation__.  For parametrized state-value functions $V(\\pi, s; \\theta)$, one particular obvious choice is $\\psi(s) = \\nabla V(\\pi, s; \\theta)$.\n",
        "\n",
        "- Suppose we estimate $\\hat \\theta$, then we can plug this in and define the \n",
        "$$\n",
        "\\hat V_{n,\\mathcal{R}} (\\pi) = \\int V(\\pi; s, \\hat \\theta) d\\mathcal{R}(s)\n",
        "$$\n",
        "where $\\mathcal{R}$ is a reference distribution (typically a distribution over initial states).\n",
        "- Then the estimated optimal regime\n",
        "$$\n",
        "\\pi_{opt} = \\arg \\max_{\\pi \\in \\Pi} \\hat V_{n,\\mathcal{R}} (\\pi)\n",
        "$$\n",
        "\n",
        "- Suppose that the state-value function is parametrized according to $V(\\pi, s; \\theta^\\pi) = \\Phi (s)^\\prime \\theta$, then define \n",
        "$$\n",
        "\\Lambda_n (\\pi, \\theta^{\\pi}) = \\left[ n^{-1} \\sum_{i=1}^n \\sum_{t=1}^{T_i} \\left( \\gamma \\Phi(S_{i,t}) \\Phi(S_{i,t+1})^\\prime - \\Phi(S_{i,t}) \\Phi(S_{i,t})^\\prime\\right) \\right] \\theta^\\pi + \n",
        "$$\n",
        "and we can estimate\n",
        "$$\n",
        "\\hat \\theta_n^\\pi = \\arg \\min_{\\theta^\\pi \\in \\Theta} \\left[ \\Lambda_n (\\pi, \\theta^{\\pi})^\\prime \\Lambda_n (\\pi, \\theta^{\\pi}) + \\lambda_n (\\theta^{\\pi})^\\prime \\theta^{\\pi} \\right]\n",
        "$$\n",
        "where $\\lambda_n$ is a tuning parameter.\n",
        "\n"
      ]
    }
  ]
}